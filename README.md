## Building a double layer RELUnet that uses Gradient Descent for optimization

> After following Fastai course and on module 3, I want to use a jupyter notebook to build a Rectified Learner Unit which is a kind of basic building block for that is used in neural nets that is used and employed in deep learning to make machine learning algorithms.

> The goal of this notebook is to explain and build the RELU while using the titanic training dataset obtained from kaggle as a test for this framework. This is built on some libraries like numpy, pytorch and some other frameworks used along the line and will be referenced as appropriate. Interesting to note here is that most of the libraries needed for this to function have all been imported from the one line from fastai.basics import * below which is as seen in the cell block below.

> RELUs are simple linear equation algorithms that uses Gradient Descent for optimizations. And that is what we are going to be doing exactly.

> The first notebook (how-does-a-neural-net-really-work) is from the fastai course and was used as a guide at some point. The two models made RELU(Reactivation Linear Units) of 1 layer and that of two layers was tested on a test dataset and the RELU1(1 layer) performed better (77%) than RELU2(two layers) (73%). Image is in the RELU notebook. The likely reason for this was referenced in the limitation part of the RELU.ipynb.

